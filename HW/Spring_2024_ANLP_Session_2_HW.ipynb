{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwEukFd8AJE7"
   },
   "source": [
    "Use the following lists to find open source data sets to complete take-home exercises. You can also apply these in the data set provided for AT1.\n",
    "\n",
    "[Open Data Sets](https://canvas.uts.edu.au/courses/32341/pages/open-data-sets-for-nlp-and-text-analysis?module_item_id=1878922)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset - https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment\n",
    "data = pd.read_csv('Datasets/Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['text'].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0                      @VirginAmerica What @dhepburn said.\\n1        @VirginAmerica plus you've added commercials t...\\n2        @VirginAmerica I didn't today... Must mean I n...\\n3        @VirginAmerica it's really aggressive to blast...\\n4        @VirginAmerica and it's a really big bad thing...\\n5        @VirginAmerica seriously would pay $30 a fligh...\\n6        @VirginAmerica yes, nearly every time I fly VX...\\n7        @VirginAmerica Really missed a prime opportuni...\\n8          @virginamerica We\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]\n",
    "\n",
    "#things to clean up - numerals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jT3X0RuyndZi"
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "1.   Calculate word associations in a large data set; try different methods to calculate it (e.g. pmi, chi-square test, etc.)\n",
    "2.  Compare lemmatization and stemming results\n",
    "3.   Try adding another pre-processing step to remove all numbers/ digits from text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ER80G_dwRkij"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ff255\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ff255\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Taken from week 2 lab\n",
    "# We create a TextPreprocessor class that encapsulates all the preprocessing steps. The class constructor allows for custom punctuation marks and stopwords to be added.\n",
    "# Each preprocessing step is implemented as a separate method so we can define in which order they need to be called.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, custom_punctuation=None, custom_stopwords=None):\n",
    "        self.punctuation = string.punctuation\n",
    "        if custom_punctuation:\n",
    "            self.punctuation += custom_punctuation\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            self.stop_words.update(custom_stopwords)\n",
    "\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return ''.join([char for char in text if char not in self.punctuation])\n",
    "\n",
    "    # Custom one for the CNN dataset - try removing below and see results\n",
    "    def add_space_after_parenthesis(self, text):\n",
    "        return re.sub(r'\\)', ') ', text)\n",
    "\n",
    "    def to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([word for word in words if word not in self.stop_words])\n",
    "\n",
    "    def remove_extra_whitespace(self, text): # This is to remove our CNN) problem - The space is added before punctuation removal, so it won't affect the final preprocessed text if you're removing all punctuation\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def stem_words(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        return ' '.join([self.stemmer.stem(word) for word in words])\n",
    "\n",
    "    # Drop the first character(is a 0) and any \\n<numeric>\n",
    "    def remove_numerics(self, text):\n",
    "        return re.sub('\\d*', '', text[1:])\n",
    "\n",
    "    #Order matters - how you call these methods is how the text will be processed step-by-step\n",
    "    # rearrange if we want to change the order of functions here\n",
    "    def preprocess(self, text):\n",
    "        text = self.add_space_after_parenthesis(text)\n",
    "        text = self.remove_numerics(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.to_lowercase(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        text = self.remove_extra_whitespace(text)\n",
    "        #text = self.stem_words(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "text_cleaned = preprocessor.preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica dhepburn said virginamerica plus youve added commercials virginamerica didnt today must mean n virginamerica really aggressive blast virginamerica really big bad thing virginamerica seriously would pay fligh virginamerica yes nearly every time fly vx virginamerica really missed prime opportuni virginamerica well didnt…but virginamerica amazing arrived virginamerica know suicide th virginamerica lt pretty graphics muc virginamerica great deal alre virginamerica virginmedia im flying'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words=word_tokenize(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virginamerica',\n",
       " 'dhepburn',\n",
       " 'said',\n",
       " 'virginamerica',\n",
       " 'plus',\n",
       " 'youve',\n",
       " 'added',\n",
       " 'commercials',\n",
       " 'virginamerica',\n",
       " 'didnt',\n",
       " 'today',\n",
       " 'must',\n",
       " 'mean',\n",
       " 'n',\n",
       " 'virginamerica',\n",
       " 'really',\n",
       " 'aggressive',\n",
       " 'blast',\n",
       " 'virginamerica',\n",
       " 'really']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aal', 'declared'),\n",
       " ('abused', 'threatened'),\n",
       " ('accommodated', 'dsm'),\n",
       " ('adamkarren', 'zj'),\n",
       " ('adolfo', 'garcia'),\n",
       " ('aggravating', 'zone'),\n",
       " ('aggressive', 'blast'),\n",
       " ('ahah😃💕🎵', 'whyni'),\n",
       " ('airlinegeeks', 'avgee'),\n",
       " ('airserv', 'contractors'),\n",
       " ('alamo', 'tat'),\n",
       " ('alert', 'immediately'),\n",
       " ('alicia', 'exceptionalservice'),\n",
       " ('americanaireveryone', 'weeksampthos'),\n",
       " ('americas', 'largest'),\n",
       " ('amiltx', 'forgiven'),\n",
       " ('amy', 'lloyd'),\n",
       " ('andyellwood', 'delk'),\n",
       " ('angriest', 'angstiest'),\n",
       " ('announcing', 'winn'),\n",
       " ('annual', 'marthas'),\n",
       " ('anticipating', 'weatherrela'),\n",
       " ('arkansas', 'gov'),\n",
       " ('aussie', 'cow'),\n",
       " ('authors', 'fiction'),\n",
       " ('avoidable', 'nonweather'),\n",
       " ('baftz', 'rcvd'),\n",
       " ('baldordash', 'rebookedarrived'),\n",
       " ('batman', 'spee'),\n",
       " ('beatriz', 'susan'),\n",
       " ('becky', 'piela'),\n",
       " ('beefjerky', 'snacks😉'),\n",
       " ('belabor', 'pointbut'),\n",
       " ('belligerent', 'jerk'),\n",
       " ('betsy', 'besty'),\n",
       " ('beware', 'barklays'),\n",
       " ('blackhistorymonth', 'commerc'),\n",
       " ('bleed', 'foot'),\n",
       " ('block', 'bike'),\n",
       " ('boom', 'httptcopzgcjchn'),\n",
       " ('bops', 'newsp'),\n",
       " ('border', 'towns'),\n",
       " ('bossf', 'waivers'),\n",
       " ('bourbon', 'street'),\n",
       " ('brandi', 'zabsonre'),\n",
       " ('brd', 'nowfrustrated'),\n",
       " ('breakdown', 'intern'),\n",
       " ('breaks', 'unloads'),\n",
       " ('breathing', 'heavily'),\n",
       " ('bright', 'side')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PMI tests\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "#fourgram_measures = nltk.collocations.QuadgramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokenized_words)\n",
    "\n",
    "#Using PMI scores to quantify and rank the BiGrams\n",
    "finder.nbest(bigram_measures.pmi, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stem_words = ' '.join([PorterStemmer().stem(word) for word in tokenized_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica dhepburn said virginamerica plu youv ad commerci virginamerica didnt today must mean n virginamerica realli aggress blast virginamerica realli big bad thing virginamerica serious would pay fligh virginamerica ye nearli everi time fli vx virginamerica realli miss prime opportuni virginamerica well didnt…but virginamerica amaz arriv virginamerica know suicid th virginamerica lt pretti graphic muc virginamerica great deal alr virginamerica virginmedia im fli f virginamerica thank virg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Library for NLP\n",
    "# Load the spacy trained pipeline to tokenize the text\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Function to lemmatize the tokens\n",
    "def lemmatize(tokens, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\"]):\n",
    "    text = \" \".join(tokens)\n",
    "    text = nlp(text)\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    for token in text:\n",
    "        if token.pos_ in allowed_postags:\n",
    "            lemmatized_tokens.append(token.lemma_)\n",
    "\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemma_words = lemmatize(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say',\n",
       " 've',\n",
       " 'add',\n",
       " 'commercial',\n",
       " 'today',\n",
       " 'mean',\n",
       " 'aggressive',\n",
       " 'blast',\n",
       " 'big',\n",
       " 'bad',\n",
       " 'thing',\n",
       " 'pay',\n",
       " 'time',\n",
       " 'fly',\n",
       " 'miss',\n",
       " 'prime',\n",
       " 'didnt',\n",
       " 'arrive',\n",
       " 'pretty',\n",
       " 'graphic',\n",
       " 'm',\n",
       " 'fly',\n",
       " 'schedule',\n",
       " 'excite',\n",
       " 'fly',\n",
       " 'last',\n",
       " '❤',\n",
       " 'fly',\n",
       " 'know',\n",
       " 'amazingl',\n",
       " 'first',\n",
       " 'fare',\n",
       " 'love',\n",
       " 'graphic',\n",
       " 'make',\n",
       " 'guy',\n",
       " 'mess',\n",
       " 'seat',\n",
       " 'happen',\n",
       " 'worry',\n",
       " 'get',\n",
       " 'seat',\n",
       " 'bked',\n",
       " 'cool',\n",
       " 'birthday',\n",
       " 'help',\n",
       " 'leave',\n",
       " 'expensive',\n",
       " 'headphone',\n",
       " 'await',\n",
       " 'return',\n",
       " 'phone',\n",
       " 'call',\n",
       " 'moodlighte',\n",
       " 'way',\n",
       " 'freddieaward',\n",
       " 'do',\n",
       " 'do',\n",
       " 'support',\n",
       " 'first',\n",
       " 'time',\n",
       " 'flyer',\n",
       " 'next',\n",
       " 'week',\n",
       " 'help',\n",
       " 'win',\n",
       " 'bid',\n",
       " 'unused',\n",
       " 'ticket',\n",
       " 'mov',\n",
       " 'flight',\n",
       " 'leave',\n",
       " 'm',\n",
       " 'elevategold',\n",
       " 'blow',\n",
       " 'way',\n",
       " 'fly',\n",
       " 'flight',\n",
       " 'leave',\n",
       " 'm',\n",
       " 'excited',\n",
       " 'know',\n",
       " 'need',\n",
       " 'm',\n",
       " 'virginamerica',\n",
       " 'new',\n",
       " 'marketing',\n",
       " 'song',\n",
       " 'call',\n",
       " 'week',\n",
       " 'try',\n",
       " 'm',\n",
       " 'hold',\n",
       " 'congrat',\n",
       " 'win',\n",
       " 'travel',\n",
       " 'fine',\n",
       " 'need',\n",
       " 'change',\n",
       " 'reservation']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQSBeMtujknV"
   },
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Try out Topic Modeling using the Sci-kit learn (SKLearn) package. There are different algorithms you can read about and experiment with - Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iI5xT7o8Rlcf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBIAVwtV_cvw"
   },
   "source": [
    "### Text Clustering\n",
    "\n",
    "Try out text clustering with a different dataset and build an optimized model by re-evaluating the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5f6rgjHXWaa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
